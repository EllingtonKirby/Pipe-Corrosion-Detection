{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset, random_split, TensorDataset\n",
    "from torchvision.transforms import functional as VF\n",
    "from torchmetrics.classification import BinaryJaccardIndex\n",
    "from torchvision import models, datasets, tv_tensors\n",
    "from torchvision.transforms import v2\n",
    "import pandas as pd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from tqdm import tqdm\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing the data - Fill NA values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = './train/images/'\n",
    "data_dict = []\n",
    "pattern = r'well_(\\d+)_patch_(\\d+)\\.npy'\n",
    "for i, filename in enumerate(os.listdir(data_dir)):\n",
    "  example = np.load(data_dir + filename)\n",
    "  match = re.match(pattern, filename)\n",
    "  if match:\n",
    "    well_number = int(match.group(1))  # Extract well number\n",
    "    patch_number = int(match.group(2)) # Extract patch number\n",
    "  else:\n",
    "    print(\"Filename format does not match the expected pattern.\")  \n",
    "  \n",
    "  data_dict.append((filename, well_number, patch_number, example.flatten()))\n",
    "\n",
    "df = pd.DataFrame(data=data_dict, columns=['filename', 'well_number', 'patch_number', 'data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_y = pd.read_csv('train/y_train.csv')\n",
    "df_y['Unnamed: 0'] = df_y['Unnamed: 0'] + '.npy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = pd.merge(df, df_y, how='left', left_on='filename', right_on='Unnamed: 0')\n",
    "data_columns = [str(i) for i in range(1296)]\n",
    "labeled_data = merged[data_columns].to_numpy()\n",
    "merged['labels'] = labeled_data.tolist()\n",
    "merged = merged.rename(columns={'Unnamed: 0':'label_name'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "matched_wells = merged[merged['label_name'].notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>16</th>\n",
       "      <th>...</th>\n",
       "      <th>1277</th>\n",
       "      <th>1278</th>\n",
       "      <th>1279</th>\n",
       "      <th>1280</th>\n",
       "      <th>1281</th>\n",
       "      <th>1282</th>\n",
       "      <th>1283</th>\n",
       "      <th>1284</th>\n",
       "      <th>1285</th>\n",
       "      <th>1286</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2530</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2533</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2534</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2535</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2537</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>671 rows Ã— 721 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        3    4    5    6    7    8    9   10   11   16  ...  1277  1278  1279  \\\n",
       "3     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   1.0   1.0   \n",
       "4     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   1.0   \n",
       "6     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   1.0   \n",
       "9     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   1.0   1.0   \n",
       "10    0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   1.0   1.0   \n",
       "...   ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...   ...   ...   ...   \n",
       "2530  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   0.0   \n",
       "2533  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   1.0   \n",
       "2534  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   1.0   1.0   \n",
       "2535  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   1.0   1.0   \n",
       "2537  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   1.0   1.0   \n",
       "\n",
       "      1280  1281  1282  1283  1284  1285  1286  \n",
       "3      1.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "4      1.0   1.0   1.0   0.0   0.0   0.0   0.0  \n",
       "6      1.0   1.0   0.0   0.0   0.0   0.0   0.0  \n",
       "9      1.0   1.0   0.0   0.0   0.0   0.0   0.0  \n",
       "10     1.0   1.0   0.0   0.0   0.0   0.0   0.0  \n",
       "...    ...   ...   ...   ...   ...   ...   ...  \n",
       "2530   1.0   1.0   0.0   0.0   0.0   0.0   0.0  \n",
       "2533   1.0   1.0   0.0   0.0   0.0   0.0   0.0  \n",
       "2534   1.0   1.0   0.0   0.0   0.0   0.0   0.0  \n",
       "2535   1.0   1.0   1.0   0.0   0.0   0.0   0.0  \n",
       "2537   1.0   1.0   0.0   0.0   0.0   0.0   0.0  \n",
       "\n",
       "[671 rows x 721 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This confirms that all values for which we have a label have at least one value above 0.0\n",
    "# So we can fill missing values with 0.0\n",
    "\n",
    "non_zero_columns = (matched_wells[data_columns] != 0.0).any()\n",
    "non_zero_columns = non_zero_columns[non_zero_columns == True]\n",
    "matched_wells[non_zero_columns.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = pd.merge(df, df_y, how='left', left_on='filename', right_on='Unnamed: 0')\n",
    "data_columns = [str(i) for i in range(1296)]\n",
    "labeled_data = merged[data_columns].to_numpy()\n",
    "labeled_data = np.nan_to_num(labeled_data)\n",
    "merged['labels'] = labeled_data.tolist()\n",
    "merged = merged.rename(columns={'Unnamed: 0':'label_name'})\n",
    "merged = merged.fillna(0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Look for outlier values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2538, 1296])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = torch.from_numpy(np.vstack(merged['data'].to_numpy(dtype=np.ndarray)))\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 175.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,\n",
       "        2363.]),\n",
       " array([-9.99250000e+02, -8.99326050e+02, -7.99402039e+02, -6.99478088e+02,\n",
       "        -5.99554077e+02, -4.99630096e+02, -3.99706116e+02, -2.99782166e+02,\n",
       "        -1.99858170e+02, -9.99341965e+01, -1.02152824e-02]),\n",
       " <BarContainer object of 10 artists>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjEAAAGdCAYAAADjWSL8AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8pXeV/AAAACXBIWXMAAA9hAAAPYQGoP6dpAAAf4UlEQVR4nO3dfWyV9f3/8deB0lK69khbew5HK9SJiitxrswCm19AsMAs6DADZGkgQYQhYAcEYS6zGgUlG5jIYIwRUEQxOlEjrFiiogTKnXRy7x23tqWg5bTwYy03n98fjis7FJBi797l+UhO4rnOu5ef88lmnzm9zjk+55wTAACAMS0aewEAAABXgogBAAAmETEAAMAkIgYAAJhExAAAAJOIGAAAYBIRAwAATCJiAACASVGNvYD6cvbsWRUXFys+Pl4+n6+xlwMAAC6Dc06VlZUKhUJq0eLSr7U024gpLi5WampqYy8DAABcgYMHD+r666+/5EyzjZj4+HhJ321CQkJCI68GAABcjoqKCqWmpnq/xy+l2UbMuT8hJSQkEDEAABhzOZeCcGEvAAAwiYgBAAAmETEAAMAkIgYAAJhExAAAAJOIGAAAYBIRAwAATCJiAACASUQMAAAwiYgBAAAmETEAAMAkIgYAAJhExAAAAJOIGAAAYFJUYy8AAABIHaauaOwl1Nq+Z+9t1H8/r8QAAACTiBgAAGASEQMAAEwiYgAAgElEDAAAMImIAQAAJhExAADAJCIGAACYRMQAAACTiBgAAGASEQMAAEwiYgAAgElEDAAAMImIAQAAJhExAADAJCIGAACYRMQAAACTiBgAAGASEQMAAEwiYgAAgElEDAAAMImIAQAAJhExAADAJCIGAACYRMQAAACTiBgAAGASEQMAAEwiYgAAgElEDAAAMImIAQAAJhExAADAJCIGAACYRMQAAACTiBgAAGASEQMAAEwiYgAAgElEDAAAMImIAQAAJhExAADAJCIGAACYRMQAAACTiBgAAGASEQMAAEwiYgAAgElEDAAAMImIAQAAJhExAADAJCIGAACYRMQAAACTahUxM2bM0M9//nPFx8crJSVF999/v/bs2RMx45xTXl6eQqGQYmNj1bNnT+3YsSNipqqqSuPHj1dycrLi4uI0cOBAHTp0KGKmvLxcOTk58vv98vv9ysnJ0bFjx67sWQIAgGanVhGzZs0aPfLIIyosLFRBQYFOnz6trKwsnThxwpuZOXOmZs2apTlz5mjTpk0KBoO65557VFlZ6c3k5uZq+fLlWrZsmdauXavjx48rOztbZ86c8WaGDRumoqIi5efnKz8/X0VFRcrJyamDpwwAAJoDn3POXekPHzlyRCkpKVqzZo3+7//+T845hUIh5ebm6rHHHpP03asugUBAzz33nEaPHq1wOKxrr71WS5Ys0ZAhQyRJxcXFSk1N1cqVK9W3b1/t2rVLt912mwoLC5WZmSlJKiwsVLdu3bR7927dcsst37u2iooK+f1+hcNhJSQkXOlTBACgQXSYuqKxl1Br+569t87PWZvf3z/omphwOCxJSkxMlCTt3btXpaWlysrK8mZiYmLUo0cPrVu3TpK0ZcsWnTp1KmImFAopPT3dm1m/fr38fr8XMJLUtWtX+f1+b+Z8VVVVqqioiLgBAIDm64ojxjmniRMn6pe//KXS09MlSaWlpZKkQCAQMRsIBLzHSktLFR0drbZt215yJiUlpca/MyUlxZs534wZM7zrZ/x+v1JTU6/0qQEAAAOuOGLGjRunTz/9VK+++mqNx3w+X8R951yNY+c7f+ZC85c6z7Rp0xQOh73bwYMHL+dpAAAAo64oYsaPH6933nlHH3zwga6//nrveDAYlKQar5aUlZV5r84Eg0FVV1ervLz8kjOHDx+u8e89cuRIjVd5zomJiVFCQkLEDQAANF+1ihjnnMaNG6c333xT77//vtLS0iIeT0tLUzAYVEFBgXesurpaa9asUffu3SVJGRkZatWqVcRMSUmJtm/f7s1069ZN4XBYGzdu9GY2bNigcDjszQAAgKtbVG2GH3nkEb3yyit6++23FR8f773i4vf7FRsbK5/Pp9zcXE2fPl0dO3ZUx44dNX36dLVp00bDhg3zZkeOHKlJkyYpKSlJiYmJmjx5sjp37qw+ffpIkjp16qR+/fpp1KhRmj9/viTp4YcfVnZ29mW9MwkAADR/tYqYefPmSZJ69uwZcXzRokUaMWKEJGnKlCk6efKkxo4dq/LycmVmZuq9995TfHy8Nz979mxFRUVp8ODBOnnypHr37q3FixerZcuW3szSpUs1YcIE711MAwcO1Jw5c67kOQIAgGboB31OTFPG58QAACzhc2K+02CfEwMAANBYiBgAAGASEQMAAEwiYgAAgElEDAAAMImIAQAAJhExAADAJCIGAACYRMQAAACTiBgAAGASEQMAAEwiYgAAgElEDAAAMImIAQAAJhExAADAJCIGAACYRMQAAACTiBgAAGASEQMAAEwiYgAAgElEDAAAMImIAQAAJhExAADAJCIGAACYRMQAAACTiBgAAGASEQMAAEwiYgAAgElEDAAAMImIAQAAJhExAADAJCIGAACYRMQAAACTiBgAAGASEQMAAEwiYgAAgElEDAAAMImIAQAAJhExAADAJCIGAACYRMQAAACTiBgAAGASEQMAAEwiYgAAgElEDAAAMImIAQAAJhExAADAJCIGAACYRMQAAACTiBgAAGASEQMAAEwiYgAAgElEDAAAMImIAQAAJhExAADAJCIGAACYRMQAAACTiBgAAGASEQMAAEwiYgAAgElEDAAAMImIAQAAJhExAADAJCIGAACYVOuI+eijjzRgwACFQiH5fD699dZbEY+PGDFCPp8v4ta1a9eImaqqKo0fP17JycmKi4vTwIEDdejQoYiZ8vJy5eTkyO/3y+/3KycnR8eOHav1EwQAAM1TrSPmxIkTuv322zVnzpyLzvTr108lJSXebeXKlRGP5+bmavny5Vq2bJnWrl2r48ePKzs7W2fOnPFmhg0bpqKiIuXn5ys/P19FRUXKycmp7XIBAEAzFVXbH+jfv7/69+9/yZmYmBgFg8ELPhYOh7Vw4UItWbJEffr0kSS9/PLLSk1N1erVq9W3b1/t2rVL+fn5KiwsVGZmpiRpwYIF6tatm/bs2aNbbrmltssGAADNTL1cE/Phhx8qJSVFN998s0aNGqWysjLvsS1btujUqVPKysryjoVCIaWnp2vdunWSpPXr18vv93sBI0ldu3aV3+/3Zs5XVVWlioqKiBsAAGi+6jxi+vfvr6VLl+r999/XX/7yF23atEl33323qqqqJEmlpaWKjo5W27ZtI34uEAiotLTUm0lJSalx7pSUFG/mfDNmzPCun/H7/UpNTa3jZwYAAJqSWv856fsMGTLE++f09HR16dJF7du314oVKzRo0KCL/pxzTj6fz7v/v/98sZn/NW3aNE2cONG7X1FRQcgAANCM1ftbrNu1a6f27dvr888/lyQFg0FVV1ervLw8Yq6srEyBQMCbOXz4cI1zHTlyxJs5X0xMjBISEiJuAACg+ar3iPnmm2908OBBtWvXTpKUkZGhVq1aqaCgwJspKSnR9u3b1b17d0lSt27dFA6HtXHjRm9mw4YNCofD3gwAALi61frPScePH9cXX3zh3d+7d6+KioqUmJioxMRE5eXl6YEHHlC7du20b98+/eEPf1BycrJ+/etfS5L8fr9GjhypSZMmKSkpSYmJiZo8ebI6d+7svVupU6dO6tevn0aNGqX58+dLkh5++GFlZ2fzziQAACDpCiJm8+bN6tWrl3f/3HUow4cP17x587Rt2za99NJLOnbsmNq1a6devXrptddeU3x8vPczs2fPVlRUlAYPHqyTJ0+qd+/eWrx4sVq2bOnNLF26VBMmTPDexTRw4MBLfjYNAAC4uvicc66xF1EfKioq5Pf7FQ6HuT4GANDkdZi6orGXUGv7nr23zs9Zm9/ffHcSAAAwiYgBAAAmETEAAMAkIgYAAJhExAAAAJOIGAAAYBIRAwAATCJiAACASUQMAAAwiYgBAAAmETEAAMAkIgYAAJhExAAAAJOIGAAAYBIRAwAATCJiAACASUQMAAAwiYgBAAAmETEAAMAkIgYAAJhExAAAAJOIGAAAYBIRAwAATCJiAACASUQMAAAwiYgBAAAmETEAAMAkIgYAAJhExAAAAJOIGAAAYBIRAwAATCJiAACASUQMAAAwiYgBAAAmETEAAMAkIgYAAJhExAAAAJOIGAAAYBIRAwAATCJiAACASUQMAAAwiYgBAAAmETEAAMAkIgYAAJhExAAAAJOIGAAAYBIRAwAATCJiAACASUQMAAAwiYgBAAAmETEAAMAkIgYAAJhExAAAAJOIGAAAYBIRAwAATCJiAACASUQMAAAwiYgBAAAmETEAAMAkIgYAAJhExAAAAJOIGAAAYBIRAwAATKp1xHz00UcaMGCAQqGQfD6f3nrrrYjHnXPKy8tTKBRSbGysevbsqR07dkTMVFVVafz48UpOTlZcXJwGDhyoQ4cORcyUl5crJydHfr9ffr9fOTk5OnbsWK2fIAAAaJ5qHTEnTpzQ7bffrjlz5lzw8ZkzZ2rWrFmaM2eONm3apGAwqHvuuUeVlZXeTG5urpYvX65ly5Zp7dq1On78uLKzs3XmzBlvZtiwYSoqKlJ+fr7y8/NVVFSknJycK3iKAACgOfI559wV/7DPp+XLl+v++++X9N2rMKFQSLm5uXrsscckffeqSyAQ0HPPPafRo0crHA7r2muv1ZIlSzRkyBBJUnFxsVJTU7Vy5Ur17dtXu3bt0m233abCwkJlZmZKkgoLC9WtWzft3r1bt9xyy/euraKiQn6/X+FwWAkJCVf6FAEAaBAdpq5o7CXU2r5n763zc9bm93edXhOzd+9elZaWKisryzsWExOjHj16aN26dZKkLVu26NSpUxEzoVBI6enp3sz69evl9/u9gJGkrl27yu/3ezPnq6qqUkVFRcQNAAA0X3UaMaWlpZKkQCAQcTwQCHiPlZaWKjo6Wm3btr3kTEpKSo3zp6SkeDPnmzFjhnf9jN/vV2pq6g9+PgAAoOmql3cn+Xy+iPvOuRrHznf+zIXmL3WeadOmKRwOe7eDBw9ewcoBAIAVdRoxwWBQkmq8WlJWVua9OhMMBlVdXa3y8vJLzhw+fLjG+Y8cOVLjVZ5zYmJilJCQEHEDAADNV51GTFpamoLBoAoKCrxj1dXVWrNmjbp37y5JysjIUKtWrSJmSkpKtH37dm+mW7duCofD2rhxozezYcMGhcNhbwYAAFzdomr7A8ePH9cXX3zh3d+7d6+KioqUmJioG264Qbm5uZo+fbo6duyojh07avr06WrTpo2GDRsmSfL7/Ro5cqQmTZqkpKQkJSYmavLkyercubP69OkjSerUqZP69eunUaNGaf78+ZKkhx9+WNnZ2Zf1ziQAAND81TpiNm/erF69enn3J06cKEkaPny4Fi9erClTpujkyZMaO3asysvLlZmZqffee0/x8fHez8yePVtRUVEaPHiwTp48qd69e2vx4sVq2bKlN7N06VJNmDDBexfTwIEDL/rZNAAA4Orzgz4npinjc2IAAJbwOTHfabTPiQEAAGgoRAwAADCJiAEAACYRMQAAwCQiBgAAmETEAAAAk4gYAABgEhEDAABMImIAAIBJRAwAADCJiAEAACYRMQAAwCQiBgAAmETEAAAAk4gYAABgEhEDAABMImIAAIBJRAwAADCJiAEAACYRMQAAwCQiBgAAmETEAAAAk4gYAABgEhEDAABMImIAAIBJRAwAADCJiAEAACYRMQAAwCQiBgAAmETEAAAAk4gYAABgEhEDAABMImIAAIBJRAwAADCJiAEAACYRMQAAwCQiBgAAmETEAAAAk4gYAABgEhEDAABMImIAAIBJRAwAADCJiAEAACYRMQAAwCQiBgAAmETEAAAAk4gYAABgEhEDAABMImIAAIBJRAwAADCJiAEAACYRMQAAwCQiBgAAmETEAAAAk4gYAABgEhEDAABMImIAAIBJRAwAADCJiAEAACYRMQAAwCQiBgAAmETEAAAAk4gYAABgEhEDAABMqvOIycvLk8/ni7gFg0Hvceec8vLyFAqFFBsbq549e2rHjh0R56iqqtL48eOVnJysuLg4DRw4UIcOHarrpQIAAMPq5ZWYn/zkJyopKfFu27Zt8x6bOXOmZs2apTlz5mjTpk0KBoO65557VFlZ6c3k5uZq+fLlWrZsmdauXavjx48rOztbZ86cqY/lAgAAg6Lq5aRRURGvvpzjnNPzzz+vxx9/XIMGDZIkvfjiiwoEAnrllVc0evRohcNhLVy4UEuWLFGfPn0kSS+//LJSU1O1evVq9e3btz6WDAAAjKmXV2I+//xzhUIhpaWlaejQofrqq68kSXv37lVpaamysrK82ZiYGPXo0UPr1q2TJG3ZskWnTp2KmAmFQkpPT/dmLqSqqkoVFRURNwAA0HzVecRkZmbqpZde0qpVq7RgwQKVlpaqe/fu+uabb1RaWipJCgQCET8TCAS8x0pLSxUdHa22bdtedOZCZsyYIb/f791SU1Pr+JkBAICmpM4jpn///nrggQfUuXNn9enTRytWrJD03Z+NzvH5fBE/45yrcex83zczbdo0hcNh73bw4MEf8CwAAEBTV+9vsY6Li1Pnzp31+eefe9fJnP+KSllZmffqTDAYVHV1tcrLyy86cyExMTFKSEiIuAEAgOar3iOmqqpKu3btUrt27ZSWlqZgMKiCggLv8erqaq1Zs0bdu3eXJGVkZKhVq1YRMyUlJdq+fbs3AwAAUOfvTpo8ebIGDBigG264QWVlZXr66adVUVGh4cOHy+fzKTc3V9OnT1fHjh3VsWNHTZ8+XW3atNGwYcMkSX6/XyNHjtSkSZOUlJSkxMRETZ482fvzFAAAgFQPEXPo0CE9+OCDOnr0qK699lp17dpVhYWFat++vSRpypQpOnnypMaOHavy8nJlZmbqvffeU3x8vHeO2bNnKyoqSoMHD9bJkyfVu3dvLV68WC1btqzr5QIAAKN8zjnX2IuoDxUVFfL7/QqHw1wfAwBo8jpMXdHYS6i1fc/eW+fnrM3vb747CQAAmETEAAAAk4gYAABgEhEDAABMImIAAIBJRAwAADCJiAEAACYRMQAAwCQiBgAAmETEAAAAk4gYAABgEhEDAABMImIAAIBJRAwAADCJiAEAACYRMQAAwCQiBgAAmETEAAAAk4gYAABgEhEDAABMImIAAIBJRAwAADCJiAEAACYRMQAAwCQiBgAAmETEAAAAk4gYAABgEhEDAABMImIAAIBJRAwAADCJiAEAACYRMQAAwCQiBgAAmETEAAAAk4gYAABgEhEDAABMImIAAIBJRAwAADCJiAEAACYRMQAAwCQiBgAAmBTV2AuwqsPUFY29hFrb9+y9jb0EAADqDK/EAAAAk4gYAABgEhEDAABMImIAAIBJRAwAADCJiAEAACYRMQAAwCQiBgAAmETEAAAAk4gYAABgEhEDAABMImIAAIBJRAwAADCJiAEAACYRMQAAwCQiBgAAmETEAAAAk4gYAABgEhEDAABMImIAAIBJRAwAADCpyUfM3LlzlZaWptatWysjI0Mff/xxYy8JAAA0AU06Yl577TXl5ubq8ccf19atW3XXXXepf//+OnDgQGMvDQAANLImHTGzZs3SyJEj9dBDD6lTp056/vnnlZqaqnnz5jX20gAAQCOLauwFXEx1dbW2bNmiqVOnRhzPysrSunXrasxXVVWpqqrKux8OhyVJFRUV9bK+s1X/r17OW5/qay8AAD8cv1ciz+mc+97ZJhsxR48e1ZkzZxQIBCKOBwIBlZaW1pifMWOGnnzyyRrHU1NT622N1vifb+wVAACak/r8vVJZWSm/33/JmSYbMef4fL6I+865Gsckadq0aZo4caJ3/+zZs/r222+VlJR0wfkrVVFRodTUVB08eFAJCQl1dl7UxF43DPa5YbDPDYN9bjj1tdfOOVVWVioUCn3vbJONmOTkZLVs2bLGqy5lZWU1Xp2RpJiYGMXExEQcu+aaa+ptfQkJCfwfpIGw1w2DfW4Y7HPDYJ8bTn3s9fe9AnNOk72wNzo6WhkZGSooKIg4XlBQoO7duzfSqgAAQFPRZF+JkaSJEycqJydHXbp0Ubdu3fT3v/9dBw4c0JgxYxp7aQAAoJE16YgZMmSIvvnmGz311FMqKSlRenq6Vq5cqfbt2zfammJiYvTEE0/U+NMV6h573TDY54bBPjcM9rnhNIW99rnLeQ8TAABAE9Nkr4kBAAC4FCIGAACYRMQAAACTiBgAAGASEfNfzzzzjLp37642bdpc9EPyDhw4oAEDBiguLk7JycmaMGGCqqurI2a2bdumHj16KDY2Vtddd52eeuqpGt//sGbNGmVkZKh169a68cYb9be//a2+npYJn332me677z4lJycrISFBv/jFL/TBBx9EzNTV3l/tVqxYoczMTMXGxio5OVmDBg2KeJx9rltVVVX66U9/Kp/Pp6KioojH2OsfZt++fRo5cqTS0tIUGxurH//4x3riiSdq7CH7XD/mzp2rtLQ0tW7dWhkZGfr4448bZyEOzjnn/vSnP7lZs2a5iRMnOr/fX+Px06dPu/T0dNerVy/3ySefuIKCAhcKhdy4ceO8mXA47AKBgBs6dKjbtm2b++c//+ni4+Pdn//8Z2/mq6++cm3atHGPPvqo27lzp1uwYIFr1aqVe+ONNxriaTZJN910k/vVr37l/v3vf7vPPvvMjR071rVp08aVlJQ45+pu7692b7zxhmvbtq2bN2+e27Nnj9u9e7d7/fXXvcfZ57o3YcIE179/fyfJbd261TvOXv9w//rXv9yIESPcqlWr3Jdffunefvttl5KS4iZNmuTNsM/1Y9myZa5Vq1ZuwYIFbufOne7RRx91cXFxbv/+/Q2+FiLmPIsWLbpgxKxcudK1aNHCff31196xV1991cXExLhwOOycc27u3LnO7/e7//znP97MjBkzXCgUcmfPnnXOOTdlyhR36623Rpx79OjRrmvXrvXwbJq+I0eOOEnuo48+8o5VVFQ4SW716tXOubrb+6vZqVOn3HXXXef+8Y9/XHSGfa5bK1eudLfeeqvbsWNHjYhhr+vHzJkzXVpamneffa4fd955pxszZkzEsVtvvdVNnTq1wdfCn5Mu0/r165Wenh7xhVR9+/ZVVVWVtmzZ4s306NEj4oN/+vbtq+LiYu3bt8+bycrKijh33759tXnzZp06dar+n0gTk5SUpE6dOumll17SiRMndPr0ac2fP1+BQEAZGRmS6m7vr2affPKJvv76a7Vo0UJ33HGH2rVrp/79+2vHjh3eDPtcdw4fPqxRo0ZpyZIlatOmTY3H2ev6EQ6HlZiY6N1nn+tedXW1tmzZUuP3WFZWltatW9fg6yFiLlNpaWmNL55s27atoqOjvS+pvNDMufvfN3P69GkdPXq0vpbfZPl8PhUUFGjr1q2Kj49X69atNXv2bOXn53vXJtXV3l/NvvrqK0lSXl6e/vjHP+rdd99V27Zt1aNHD3377beS2Oe64pzTiBEjNGbMGHXp0uWCM+x13fvyyy/1wgsvRHwtDftc944ePaozZ85ccM8aY7+adcTk5eXJ5/Nd8rZ58+bLPp/P56txzDkXcfz8Gfffi8NqO2Pd5e69c05jx45VSkqKPv74Y23cuFH33XefsrOzVVJS4p2vrva+ubncfT579qwk6fHHH9cDDzygjIwMLVq0SD6fT6+//rp3Pvb54i53r1944QVVVFRo2rRplzwfe31hV/Lf7eLiYvXr10+/+c1v9NBDD0U8xj7XjwvtWWPsV5P+7qQfaty4cRo6dOglZzp06HBZ5woGg9qwYUPEsfLycp06dcor0mAwWKNEy8rKJOl7Z6KiopSUlHRZa7Hgcvf+/fff17vvvqvy8nLvq9znzp2rgoICvfjii5o6dWqd7X1zdLn7XFlZKUm67bbbvOMxMTG68cYbdeDAAUl197/x5upy9/rpp59WYWFhje+T6dKli37729/qxRdfZK8vobb/3S4uLlavXr28Lwn+X+xz3UtOTlbLli0vuGeNsl8NfhVOE/d9F/YWFxd7x5YtW1bjArFrrrnGVVVVeTPPPvtsjQt7O3XqFHHuMWPGXLUX9r7zzjuuRYsWrrKyMuL4zTff7J555hnnXN3t/dUsHA67mJiYiAt7q6urXUpKips/f75zjn2uK/v373fbtm3zbqtWrXKS3BtvvOEOHjzonGOv68qhQ4dcx44d3dChQ93p06drPM4+148777zT/e53v4s41qlTp0a5sJeI+a/9+/e7rVu3uieffNL96Ec/clu3bnVbt271frmee6te79693SeffOJWr17trr/++oi36h07dswFAgH34IMPum3btrk333zTJSQkXPAt1r///e/dzp073cKFC6/qt1gfOXLEJSUluUGDBrmioiK3Z88eN3nyZNeqVStXVFTknKu7vb/aPfroo+66665zq1atcrt373YjR450KSkp7ttvv3XOsc/1Ze/evRd9izV7feW+/vprd9NNN7m7777bHTp0yJWUlHi3c9jn+nHuLdYLFy50O3fudLm5uS4uLs7t27evwddCxPzX8OHDnaQatw8++MCb2b9/v7v33ntdbGysS0xMdOPGjYt4W55zzn366afurrvucjExMS4YDLq8vLwaNf/hhx+6O+64w0VHR7sOHTq4efPmNcRTbLI2bdrksrKyXGJioouPj3ddu3Z1K1eujJipq72/mlVXV7tJkya5lJQUFx8f7/r06eO2b98eMcM+170LRYxz7PUPtWjRogv+N/v8PzCwz/Xjr3/9q2vfvr2Ljo52P/vZz9yaNWsaZR0+5/hYQgAAYE+zfncSAABovogYAABgEhEDAABMImIAAIBJRAwAADCJiAEAACYRMQAAwCQiBgAAmETEAAAAk4gYAABgEhEDAABMImIAAIBJ/x9FjQRyUVn/MAAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(data.min(dim=1, keepdim=True).values.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2538])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outliers = ((data.min(dim=1, keepdim=True).values == -999.2500) == True).flatten()\n",
    "outliers.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = merged.drop(merged.loc[outliers.tolist()].index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA not available, switching to MPS\n"
     ]
    }
   ],
   "source": [
    "global DEVICE\n",
    "DEVICE = torch.device('cpu')\n",
    "if torch.cuda.is_available():\n",
    "    DEVICE = torch.device('cuda:0')\n",
    "    print(\"CUDA is available and is used\")\n",
    "elif not torch.backends.mps.is_available():\n",
    "    if not torch.backends.mps.is_built():\n",
    "        print(\"MPS not available because the current PyTorch install was not \"\n",
    "            \"built with MPS enabled.\")\n",
    "    else:\n",
    "        print(\"MPS not available because the current MacOS version is not 12.3+ \"\n",
    "            \"and/or you do not have an MPS-enabled device on this machine.\")\n",
    "    DEVICE = torch.device('cpu')\n",
    "    print(\"CUDA and MPS are not available, switching to CPU.\")\n",
    "else:\n",
    "    DEVICE = torch.device(\"mps\")\n",
    "    print(\"CUDA not available, switching to MPS\")\n",
    "\n",
    "\n",
    "class WellsDataset(Dataset):\n",
    "    def __init__(self, data, labels, transform):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "        self.target_transform = False\n",
    "        self.scaler = RobustScaler()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.data[idx]\n",
    "        label = self.labels[idx]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "        return image, label\n",
    "    \n",
    "def build_dataframe():\n",
    "    if os.path.exists('./inputs/x_train_df.csv'):\n",
    "        return pd.read_csv('inputs/x_train_df.csv')\n",
    "\n",
    "    data_dir = './train/images/'\n",
    "    data_dict = []\n",
    "    pattern = r'well_(\\d+)_patch_(\\d+)\\.npy'\n",
    "    for i, filename in enumerate(os.listdir(data_dir)):\n",
    "        example = np.load(data_dir + filename)\n",
    "        match = re.match(pattern, filename)\n",
    "        if match:\n",
    "            well_number = int(match.group(1))  # Extract well number\n",
    "            patch_number = int(match.group(2)) # Extract patch number\n",
    "        else:\n",
    "            print(\"Filename format does not match the expected pattern.\")  \n",
    "\n",
    "        data_dict.append((filename, well_number, patch_number, example.flatten()))\n",
    "\n",
    "    df = pd.DataFrame(data=data_dict, columns=['filename', 'well_number', 'patch_number', 'data'])\n",
    "\n",
    "    df_y = pd.read_csv('train/y_train.csv')\n",
    "    df_y['Unnamed: 0'] = df_y['Unnamed: 0'] + '.npy'\n",
    "\n",
    "    # Create single dataframe with data and labels as lists\n",
    "    merged = pd.merge(df, df_y, how='left', left_on='filename', right_on='Unnamed: 0')\n",
    "    data_columns = [str(i) for i in range(1296)]\n",
    "    labeled_data = merged[data_columns].to_numpy()\n",
    "    # Convert missing labels to all zeros\n",
    "    labeled_data = np.nan_to_num(labeled_data)\n",
    "    merged['labels'] = labeled_data.tolist()\n",
    "    merged = merged.rename(columns={'Unnamed: 0':'label_name'})\n",
    "    merged = merged.fillna(0.0)\n",
    "\n",
    "    data = torch.from_numpy(np.vstack(merged['data'].to_numpy(dtype=np.ndarray)))\n",
    "    # Remove corruputed samples\n",
    "    outliers = ((data.min(dim=1, keepdim=True).values == -999.2500) == True).flatten()\n",
    "\n",
    "    merged = merged.drop(merged.loc[outliers.tolist()].index)\n",
    "    merged.to_csv(path_or_buf='./inputs/x_train.csv')\n",
    "    return merged\n",
    "\n",
    "def build_dataloaders(dataframe):\n",
    "  data = torch.from_numpy(np.vstack(dataframe['data'].to_numpy())).reshape(-1, 1, 36, 36)\n",
    "  data = torch.nan_to_num(data)\n",
    "  labels = torch.from_numpy(np.vstack(dataframe['labels'].to_numpy())).reshape(-1, 1, 36, 36)\n",
    "\n",
    "  p = np.random.permutation(len(data))\n",
    "  data, labels = data[p], labels[p]\n",
    "\n",
    "  offset = int(len(data) * .8)\n",
    "  X_train, X_valid = data[:offset], data[offset:]\n",
    "  Y_train, Y_valid = labels[:offset].float(), labels[offset:].float()\n",
    "\n",
    "  # scaler = RobustScaler()\n",
    "  # scaler.fit(X_train)\n",
    "  # X_train = torch.tensor(scaler.transform(X_train)).float()\n",
    "  # X_valid = torch.tensor(scaler.transform(X_valid)).float()\n",
    "  \n",
    "  rolled_x, rolled_y = [], []\n",
    "  for i in range(36, 2):\n",
    "      rolled_x.append(torch.roll(X_train, i, dims=3))\n",
    "      rolled_y.append(torch.roll(Y_train, i, dims=3))\n",
    "\n",
    "  X_train, Y_train = torch.vstack((X_train, *rolled_x)), torch.vstack((Y_train, *rolled_y))\n",
    "\n",
    "  flipper = v2.RandomVerticalFlip(1)\n",
    "  X_train, Y_train = torch.vstack((X_train, flipper(X_train))), torch.vstack((Y_train, flipper(Y_train)))\n",
    "  \n",
    "  train_dataset = WellsDataset(X_train, Y_train, None)\n",
    "  valid_dataset = WellsDataset(X_valid, Y_valid, None)\n",
    "\n",
    "  train_dataloader = DataLoader(train_dataset, batch_size=128)\n",
    "  valid_dataloader = DataLoader(valid_dataset, batch_size=128)\n",
    "\n",
    "  return train_dataloader, valid_dataloader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# View Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl, valid_dl = build_dataloaders(build_dataframe())\n",
    "samples, labels = next(iter(train_dl))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implement the Baseline Model\n",
    "\n",
    "- The CNN architecture consists of 5 layers without pooling.\n",
    "- A batch size of 128 was chosen to optimize computational efficiency during the training process.\n",
    "- The learning rate was set at 0.001 to guide the model through effective convergence.\n",
    "- Training was conducted over 30 epochs to capture the temporal evolution of features within the data.\n",
    "- Utilizing the Binary Cross Entropy loss function facilitated effective optimization, striking a balance between the dice coefficient and the binary cross-entropy components.\n",
    "- Data augmentation techniques, including flip and horizontal roll, were strategically incorporated to enhance the model's adaptability.\n",
    "- The optimizer used during training was the Adam optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Baseline(nn.Module):\n",
    "  def __init__(self) -> None:\n",
    "    super().__init__()\n",
    "    num_channels = 64\n",
    "    self.feature_extractor = nn.Sequential(\n",
    "      nn.Conv2d(in_channels=1, out_channels=num_channels, kernel_size=3, padding=1),\n",
    "      nn.ReLU(),\n",
    "      nn.Conv2d(in_channels=num_channels, out_channels=num_channels, kernel_size=3, padding=1),\n",
    "      nn.ReLU(),\n",
    "      nn.Conv2d(in_channels=num_channels, out_channels=num_channels*2, kernel_size=3, padding=1),\n",
    "      nn.ReLU(),\n",
    "      nn.Conv2d(in_channels=num_channels*2, out_channels=num_channels*2, kernel_size=3, padding=1),\n",
    "      nn.ReLU(),\n",
    "      nn.Conv2d(in_channels=num_channels*2, out_channels=1, kernel_size=3, padding=1),\n",
    "      nn.ReLU(),\n",
    "    )\n",
    "    self.classifier = nn.Sequential(\n",
    "      nn.Sigmoid()\n",
    "    )\n",
    "  \n",
    "  def forward(self, input):\n",
    "    x = self.feature_extractor(input)\n",
    "    return self.classifier(x)\n",
    "  \n",
    "class DiceLoss(nn.Module):\n",
    "    def __init__(self, weight=None, size_average=True):\n",
    "        super(DiceLoss, self).__init__()\n",
    "\n",
    "    def forward(self, inputs, targets, smooth=1):\n",
    "        \n",
    "        #comment out if your model contains a sigmoid or equivalent activation layer\n",
    "        # inputs = F.sigmoid(inputs)       \n",
    "        \n",
    "        #flatten label and prediction tensors\n",
    "        inputs = inputs.view(-1)\n",
    "        targets = targets.view(-1)\n",
    "        \n",
    "        intersection = (inputs * targets).sum()                            \n",
    "        dice = (2.*intersection + smooth)/(inputs.sum() + targets.sum() + smooth)  \n",
    "        \n",
    "        return 1 - dice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_dataloaer, validation_dataloader, num_epochs, lr):\n",
    "  model = Baseline().to(DEVICE)\n",
    "  optimizer = torch.optim.Adam(lr=lr, params=model.parameters())\n",
    "  metric = BinaryJaccardIndex().to(DEVICE)\n",
    "  criterion = DiceLoss().to(DEVICE)\n",
    "  for e in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    train_iou = 0\n",
    "    for input, labels in tqdm(iter(train_dataloaer)):\n",
    "      input = input.to(DEVICE)\n",
    "      labels = labels.to(DEVICE)\n",
    "      optimizer.zero_grad()\n",
    "      output = model(input)\n",
    "      loss = criterion(output, labels)\n",
    "      train_loss += loss.detach().item()\n",
    "      iou = metric(output, labels)\n",
    "      train_iou += iou.detach().item()\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "\n",
    "    model.eval()\n",
    "    valid_loss = 0\n",
    "    valid_iou = 0\n",
    "    with torch.no_grad():\n",
    "      for input, labels in tqdm(iter(validation_dataloader)):\n",
    "        input = input.to(DEVICE)\n",
    "        labels = labels.to(DEVICE)\n",
    "        out = model(input)\n",
    "        loss = criterion(out, labels)\n",
    "        iou = metric(out, labels)\n",
    "        valid_loss += loss.detach().item()\n",
    "        valid_iou += iou.detach().item()\n",
    "    \n",
    "    print(f'Epoch: {e}')\n",
    "    print(f'Train loss:      {train_loss / len(train_dataloaer)}')\n",
    "    print(f'Validation loss: {valid_loss / len(validation_dataloader)}')\n",
    "    print(f'Train intersection over union:      {train_iou}')\n",
    "    print(f'Validation intersection over union: {valid_iou}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "    def __init__(self, n_channels, n_classes, bilinear=False):\n",
    "        super(UNet, self).__init__()\n",
    "        self.n_channels = n_channels\n",
    "        self.n_classes = n_classes\n",
    "        self.bilinear = bilinear\n",
    "\n",
    "        self.inc = (DoubleConv(n_channels, 64))\n",
    "        self.down1 = (Down(64, 128))\n",
    "        self.down2 = (Down(128, 256))\n",
    "        # self.down3 = (Down(256, 512))\n",
    "        factor = 2 if bilinear else 1\n",
    "        # self.down4 = (Down(512, 1024 // factor))\n",
    "        # self.up1 = (Up(1024, 512 // factor, bilinear))\n",
    "        # self.up2 = (Up(512, 256 // factor, bilinear))\n",
    "        self.up3 = (Up(256, 128 // factor, bilinear))\n",
    "        self.up4 = (Up(128, 64, bilinear))\n",
    "        self.outc = (OutConv(64, n_classes))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.inc(x)\n",
    "        x2 = self.down1(x1)\n",
    "        x3 = self.down2(x2)\n",
    "        # x4 = self.down3(x3)\n",
    "        # x5 = self.down4(x4)\n",
    "        # x = self.up1(x5, x4)\n",
    "        # x = self.up2(x, x3)\n",
    "        x = self.up3(x3, x2)\n",
    "        x = self.up4(x, x1)\n",
    "        logits = self.outc(x)\n",
    "        return logits\n",
    "\n",
    "    # def use_checkpointing(self):\n",
    "    #     self.inc = torch.utils.checkpoint(self.inc)\n",
    "    #     self.down1 = torch.utils.checkpoint(self.down1)\n",
    "    #     self.down2 = torch.utils.checkpoint(self.down2)\n",
    "    #     self.down3 = torch.utils.checkpoint(self.down3)\n",
    "    #     self.down4 = torch.utils.checkpoint(self.down4)\n",
    "    #     self.up1 = torch.utils.checkpoint(self.up1)\n",
    "    #     self.up2 = torch.utils.checkpoint(self.up2)\n",
    "    #     self.up3 = torch.utils.checkpoint(self.up3)\n",
    "    #     self.up4 = torch.utils.checkpoint(self.up4)\n",
    "    #     self.outc = torch.utils.checkpoint(self.outc)\n",
    "\n",
    "class DoubleConv(nn.Module):\n",
    "    \"\"\"(convolution => [BN] => ReLU) * 2\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, mid_channels=None):\n",
    "        super().__init__()\n",
    "        if not mid_channels:\n",
    "            mid_channels = out_channels\n",
    "        self.double_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, mid_channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(mid_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(mid_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.double_conv(x)\n",
    "\n",
    "\n",
    "class Down(nn.Module):\n",
    "    \"\"\"Downscaling with maxpool then double conv\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.maxpool_conv = nn.Sequential(\n",
    "            nn.MaxPool2d(2),\n",
    "            DoubleConv(in_channels, out_channels)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.maxpool_conv(x)\n",
    "\n",
    "\n",
    "class Up(nn.Module):\n",
    "    \"\"\"Upscaling then double conv\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, bilinear=True):\n",
    "        super().__init__()\n",
    "\n",
    "        # if bilinear, use the normal convolutions to reduce the number of channels\n",
    "        if bilinear:\n",
    "            self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "            self.conv = DoubleConv(in_channels, out_channels, in_channels // 2)\n",
    "        else:\n",
    "            self.up = nn.ConvTranspose2d(in_channels, in_channels // 2, kernel_size=2, stride=2)\n",
    "            self.conv = DoubleConv(in_channels, out_channels)\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        x1 = self.up(x1)\n",
    "        # input is CHW\n",
    "        diffY = x2.size()[2] - x1.size()[2]\n",
    "        diffX = x2.size()[3] - x1.size()[3]\n",
    "\n",
    "        x1 = F.pad(x1, [diffX // 2, diffX - diffX // 2,\n",
    "                        diffY // 2, diffY - diffY // 2])\n",
    "        # if you have padding issues, see\n",
    "        # https://github.com/HaiyongJiang/U-Net-Pytorch-Unstructured-Buggy/commit/0e854509c2cea854e247a9c615f175f76fbb2e3a\n",
    "        # https://github.com/xiaopeng-liao/Pytorch-UNet/commit/8ebac70e633bac59fc22bb5195e513d5832fb3bd\n",
    "        x = torch.cat([x2, x1], dim=1)\n",
    "        return self.conv(x)\n",
    "\n",
    "\n",
    "class OutConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(OutConv, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = UNet(n_channels=1, n_classes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = torch.rand((1, 1,36,36))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2, 36, 36])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 36, 36])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.argmax(F.softmax(output, dim=1), dim=1).view(-1, 1, 36, 36).shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
