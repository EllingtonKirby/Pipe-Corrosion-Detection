{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset, random_split, TensorDataset\n",
    "from torchvision.transforms import functional as VF\n",
    "from torchmetrics.classification import BinaryJaccardIndex\n",
    "from torchvision import models, datasets, tv_tensors\n",
    "from torchvision.transforms import v2\n",
    "import pandas as pd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from tqdm import tqdm\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WellsDataset(Dataset):\n",
    "    def __init__(self, data, labels, transform):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "        self.target_transform = False\n",
    "        self.scaler = RobustScaler()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.data[idx]\n",
    "        label = self.labels[idx]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "        return image, label\n",
    "def build_train_dataframe():\n",
    "  if os.path.exists('./inputs/x_train_df.csv'):\n",
    "      return pd.read_csv('inputs/x_train_df.csv')\n",
    "\n",
    "  data_dir = './train/images/'\n",
    "  data_dict = []\n",
    "  pattern = r'well_(\\d+)_patch_(\\d+)\\.npy'\n",
    "  for i, filename in enumerate(os.listdir(data_dir)):\n",
    "      example = np.load(data_dir + filename)\n",
    "      match = re.match(pattern, filename)\n",
    "      if match:\n",
    "          well_number = int(match.group(1))  # Extract well number\n",
    "          patch_number = int(match.group(2)) # Extract patch number\n",
    "      else:\n",
    "          print(\"Filename format does not match the expected pattern.\")  \n",
    "\n",
    "      data_dict.append((filename, well_number, patch_number, example.flatten()))\n",
    "\n",
    "  df = pd.DataFrame(data=data_dict, columns=['filename', 'well_number', 'patch_number', 'data'])\n",
    "\n",
    "  df_y = pd.read_csv('train/y_train.csv')\n",
    "  df_y['Unnamed: 0'] = df_y['Unnamed: 0'] + '.npy'\n",
    "\n",
    "  # Create single dataframe with data and labels as lists\n",
    "  merged = pd.merge(df, df_y, how='left', left_on='filename', right_on='Unnamed: 0')\n",
    "  data_columns = [str(i) for i in range(1296)]\n",
    "  labeled_data = merged[data_columns].to_numpy()\n",
    "  # Convert missing labels to all zeros\n",
    "  labeled_data = np.nan_to_num(labeled_data)\n",
    "  merged['labels'] = labeled_data.tolist()\n",
    "  merged = merged.rename(columns={'Unnamed: 0':'label_name'})\n",
    "  merged = merged.fillna(0.0)\n",
    "\n",
    "  data = torch.from_numpy(np.vstack(merged['data'].to_numpy(dtype=np.ndarray)))\n",
    "  # Remove corruputed samples\n",
    "  outliers = ((data.min(dim=1, keepdim=True).values == -999.2500) == True).flatten()\n",
    "\n",
    "  merged = merged.drop(merged.loc[outliers.tolist()].index)\n",
    "  merged.to_csv(path_or_buf='./inputs/x_train.csv')\n",
    "  return merged.sort_values(by=['well_number','patch_number'])\n",
    "\n",
    "def build_test_dataframe():\n",
    "  data_dir = './test/images/'\n",
    "  data_dict = []\n",
    "  pattern = r'well_(\\d+)_patch_(\\d+)\\.npy'\n",
    "  for i, filename in enumerate(os.listdir(data_dir)):\n",
    "      example = np.load(data_dir + filename)\n",
    "      match = re.match(pattern, filename)\n",
    "      if match:\n",
    "          well_number = int(match.group(1))  # Extract well number\n",
    "          patch_number = int(match.group(2)) # Extract patch number\n",
    "      else:\n",
    "          print(\"Filename format does not match the expected pattern.\")  \n",
    "\n",
    "      data_dict.append((filename, well_number, patch_number, example.flatten()))\n",
    "\n",
    "  df = pd.DataFrame(data=data_dict, columns=['filename', 'well_number', 'patch_number', 'data'])\n",
    "  return df.sort_values(by=['well_number','patch_number'])\n",
    "\n",
    "def build_dataloaders(test_dataframe, train_dataframe):\n",
    "  test_data = torch.from_numpy(np.vstack(test_dataframe['data'].to_numpy()))\n",
    "  test_data = torch.nan_to_num(test_data)\n",
    "  \n",
    "  train_data = torch.from_numpy(np.vstack(train_dataframe['data'].to_numpy()))\n",
    "  train_data = torch.nan_to_num(train_data)\n",
    "\n",
    "  scaler = RobustScaler()\n",
    "  scaler.fit(train_data)\n",
    "  \n",
    "  X_test = torch.tensor(scaler.transform(test_data)).float().reshape(-1, 1, 36, 36)\n",
    "  X_train = torch.tensor(scaler.transform(train_data)).float().reshape(-1, 1, 36, 36)\n",
    "  return X_test, X_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unet\n",
    "model = unet.UNet(n_channels=1, n_classes=1)\n",
    "model.load_state_dict(torch.load('./unet_1.pt', map_location=torch.device('cpu')))\n",
    "model.eval()\n",
    "test_df = build_test_dataframe()\n",
    "train_df = build_train_dataframe()\n",
    "X_test, X_train = build_dataloaders(test_df, train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "well_5_train = train_df[train_df['well_number'] == 5]['data']\n",
    "well_5_train = torch.from_numpy(np.vstack(well_5_train.values)).reshape(-1, 1, 36, 36)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "well_5_labels = train_df[train_df['well_number'] == 5]['labels']\n",
    "well_5_labels = torch.from_numpy(np.vstack(well_5_labels.values)).reshape(-1, 1, 36, 36).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/wAALCAAkACQBAREA/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/9oACAEBAAA/APn+iiiiiiiiiiiiiiiiiiiiiiiiv//Z",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAACQAAAAkCAAAAADEa8dEAAAAGElEQVR4AWNgGAWjITAaAqMhMBoCxIYAAAU0AAFCwR6cAAAAAElFTkSuQmCC",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=36x36>"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = (F.sigmoid(model(torch.from_numpy(train_df.loc[300]['data']).reshape(-1,1,36,36))) > .5)*1.\n",
    "VF.to_pil_image(preds.view(-1, 36, 36))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/wAALCAAkACQBAREA/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/9oACAEBAAA/APn+vQPgl/yV7Qv+3j/0nkroP2jv+Sh6f/2Co/8A0bLXj9FFegfBL/kr2hf9vH/pPJVz486lNffFS8t5VjCWFvDbxFQclSgly3PXdIw4xwB9T5nRRXonwOgmm+LekPFFI6QpO8rKpIRfJdct6DcyjJ7kDvUfxt/5K9rv/bv/AOk8def0UV7B+zj/AMlD1D/sFSf+jYq5/wCNv/JXtd/7d/8A0njrz+iivYP2cf8Akoeof9gqT/0bFXP/ABt/5K9rv/bv/wCk8def0V//2Q==",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAACQAAAAkCAAAAADEa8dEAAAAO0lEQVR4AWNkQAH/QTxGFCEghwldABt/VBFtwwkjVmhrHWYUj0YwOSEOzlGjgYkZAuQEJhZTQEJ0TpkAup8CSEyE+pgAAAAASUVORK5CYII=",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=36x36>"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label = torch.from_numpy(np.vstack(train_df.loc[300]['labels'])).reshape(1,36,36).float()\n",
    "VF.to_pil_image(label)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
